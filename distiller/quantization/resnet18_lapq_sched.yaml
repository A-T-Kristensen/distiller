# Sample quantization coordinate search (using Powell's method) configuration.
# This file defines the quantization config, in this case we skip some of the layers
# and all the other layers are quantized 4 bit on both weights and activations.
#
# To invoke the search, run:
#
# python pqt_coordinate_search.py -a resnet18 --pretrained -j 22 /data/datasets/imagenet --opt-val-size 3.7e-3 --opt-maxiter 2 -b 500 --opt-init-mode L3 --opt-eval-memoize-dataloader --qe-config-file resnet18_sched.yaml --opt-search-clipping
# (Note that when '--qe-config-file' is passed, all other '--qe*' arguments are ignored. Only the settings in the YAML file are used)
#
# Specifically, configuring with a YAML file allows us to define the 'overrides' section, which is cumbersome
# to define programmatically and not exposed as a command-line argument.
#
# To illustrate how this may come in handy, we'll try post-training quantization of ResNet-18 using 6-bits for weights
# and activations. First we'll see what we get when we quantize all layers with 6-bits, and then we'll see how we
# can get better results by selectively quantizing some layers to 8-bits.
#
# +-----+----------+------------------------------------------+--------+--------+-----------------------------------+
# | Run | Num Bits |                 Overrides                | Top-1  | Diff   | How-To                            |
# + No. + Default  +------------------------------------------+        +        +                                   +
# |     |          | Num Bits     | Num Bits | Clip output of |        |        |                                   |
# |     |          | First & Last | EltWise  | final layer    |        |        |                                   |
# |     |          | Layers       | Adds     | before softmax |        |        |                                   |
# +-----+----------+--------------+----------+----------------+--------+--------+-----------------------------------+
# | 1   | FP32     | N/A          | N/A      | N/A            | 69.758 |        | Run command line without part     |
# |     |          |              |          |                |        |        | starting with '--quantize-eval'   |
# +-----+----------+--------------+----------+----------------+--------+--------+-----------------------------------+
# | 2   | 6        | 6            | 6        | Yes            | 62.776 | -6.982 | Comment out all override sections |
# |     |          |              |          |                |        |        | Run full command line             |
# +-----+----------+--------------+----------+----------------+--------+--------+-----------------------------------+
# | 3   | 6        | 6            | 6        | No             | 67.748 | −2.01  | Uncomment override section for    |
# +-----+----------+--------------+----------+----------------+--------+--------+ the specific run (all others      +
# | 4   | 6        | 6            | 8        | No             | 67.928 | −1.83  | commented out)                    |
# +-----+----------+--------------+----------+----------------+--------+--------+                                   +
# | 5   | 6        | 8            | 6        | No             | 68.872 | −0.886 | Run full command line             |
# +-----+----------+--------------+----------+----------------+--------+--------+                                   +
# | 6   | 6        | 8            | 8        | No             | 68.976 | −0.782 |                                   |
# +-----+----------+--------------+----------+----------------+--------+--------+-----------------------------------+
#
# We can see that the largest boost to accuracy, ~5%, is obtained by disabling activations clipping for the final layer
# Quantizing the first and last layers to 8 bits instead of 6 bits boosts accuracy by another ~1.1%
# Quantizing the element-wise add layers to 8-bits gives another small boost of ~0.2%

quantizers:
  post_train_quantizer:
    class: PostTrainLinearQuantizer
    bits_activations: 5
    bits_parameters: 4
    bits_accum: 32
    mode: SYMMETRIC
    # Path to stats file assuming this is being invoked from the 'classifier_compression' example directory
    model_activation_stats: ../../examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml
    per_channel_wts: false
    clip_acts: LAPLACE
    save_fp_weights: true
    overrides:
      conv1:
        bits_weights: null
        bits_activations: null
      maxpool:
        bits_activations: null
      relu:
        bits_activations: null
      .*conv2:
        bits_weights: null
        bits_activations: null
      .*add:
        fake: True
      avgpool:
        bits_weights: null
        bits_activations: null
      fc:
        bits_weights: null
        bits_activations: null

